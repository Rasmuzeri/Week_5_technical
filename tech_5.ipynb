{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:03:43.117699Z","iopub.execute_input":"2025-02-19T11:03:43.118019Z","iopub.status.idle":"2025-02-19T11:04:08.390911Z","shell.execute_reply.started":"2025-02-19T11:03:43.117990Z","shell.execute_reply":"2025-02-19T11:04:08.389850Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nydata-profiling 4.12.1 requires scipy<1.14,>=1.4.1, but you have scipy 1.15.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q scipy==1.13.0 scikit-learn==1.3.1 matplotlib==3.8.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:04:08.392212Z","iopub.execute_input":"2025-02-19T11:04:08.392499Z","iopub.status.idle":"2025-02-19T11:04:33.635787Z","shell.execute_reply.started":"2025-02-19T11:04:08.392466Z","shell.execute_reply":"2025-02-19T11:04:33.634949Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import bitsandbytes\nimport transformers\nimport peft\nimport accelerate\nimport datasets\nimport scipy\nimport einops\nimport evaluate\nimport trl\nimport rouge_score\n\nprint(\"BitsAndBytes version:\", bitsandbytes.__version__)\nprint(\"Transformers version:\", transformers.__version__)\nprint(\"PEFT version:\", peft.__version__)\nprint(\"Accelerate version:\", accelerate.__version__)\nprint(\"Datasets version:\", datasets.__version__)\nprint(\"Scipy version:\", scipy.__version__)\nprint(\"Einops version:\", einops.__version__)\nprint(\"Evaluate version:\", evaluate.__version__)\nprint(\"TRL version:\", trl.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:04:33.637687Z","iopub.execute_input":"2025-02-19T11:04:33.637996Z","iopub.status.idle":"2025-02-19T11:04:53.098572Z","shell.execute_reply.started":"2025-02-19T11:04:33.637963Z","shell.execute_reply":"2025-02-19T11:04:53.097706Z"}},"outputs":[{"name":"stdout","text":"BitsAndBytes version: 0.45.2\nTransformers version: 4.49.0\nPEFT version: 0.14.0\nAccelerate version: 1.4.0\nDatasets version: 3.3.1\nScipy version: 1.13.0\nEinops version: 0.8.1\nEvaluate version: 0.4.3\nTRL version: 0.15.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:04:53.100038Z","iopub.execute_input":"2025-02-19T11:04:53.100650Z","iopub.status.idle":"2025-02-19T11:04:53.104156Z","shell.execute_reply.started":"2025-02-19T11:04:53.100598Z","shell.execute_reply":"2025-02-19T11:04:53.103276Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:04:53.105218Z","iopub.execute_input":"2025-02-19T11:04:53.105499Z","iopub.status.idle":"2025-02-19T11:04:57.734078Z","shell.execute_reply.started":"2025-02-19T11:04:53.105471Z","shell.execute_reply":"2025-02-19T11:04:57.733147Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  路路路路路路路路\nAdd token as git credential? (Y/n)  n\n"}],"execution_count":5},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:04:57.734842Z","iopub.execute_input":"2025-02-19T11:04:57.735069Z","iopub.status.idle":"2025-02-19T11:04:57.739366Z","shell.execute_reply.started":"2025-02-19T11:04:57.735050Z","shell.execute_reply":"2025-02-19T11:04:57.738391Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# https://huggingface.co/datasets/neil-code/dialogsum-test\nhuggingface_dataset_name = \"neil-code/dialogsum-test\"\ndataset = load_dataset(huggingface_dataset_name)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:04:57.740054Z","iopub.execute_input":"2025-02-19T11:04:57.740309Z","iopub.status.idle":"2025-02-19T11:05:03.191316Z","shell.execute_reply.started":"2025-02-19T11:04:57.740290Z","shell.execute_reply":"2025-02-19T11:05:03.190702Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"020c0b860ed9454baadb4e9292fec2ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d38cb5344cf14e9fb1724cd2a46bc305"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/441k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"292d1b8e95cd4eb8b5a1ce71c0ca39c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f25932d7a94e45ed838d11f977df78ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51fba982b6134851908b1de05aee9517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f62ae7db0ab4ed6abb129fd325221b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8382464fcb64a1da9949400ccb72861"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1999\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"dataset['train'][0]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:05:03.193731Z","iopub.execute_input":"2025-02-19T11:05:03.193946Z","iopub.status.idle":"2025-02-19T11:05:03.201422Z","shell.execute_reply.started":"2025-02-19T11:05:03.193927Z","shell.execute_reply":"2025-02-19T11:05:03.200677Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\ndevice_map = {\"\": 0}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:05:03.203022Z","iopub.execute_input":"2025-02-19T11:05:03.203237Z","iopub.status.idle":"2025-02-19T11:05:03.216442Z","shell.execute_reply.started":"2025-02-19T11:05:03.203216Z","shell.execute_reply":"2025-02-19T11:05:03.215690Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model_name='microsoft/phi-2'\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:05:03.217234Z","iopub.execute_input":"2025-02-19T11:05:03.217517Z","iopub.status.idle":"2025-02-19T11:07:20.429184Z","shell.execute_reply.started":"2025-02-19T11:05:03.217489Z","shell.execute_reply":"2025-02-19T11:07:20.428346Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f6c005c761f4ecba9e74318eacc00ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"511160f6032745bdb48a2ca51223339d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca29af11208349e8853cc081be9c5fa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6a8d85d24094118805940139f74de40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb2c553a25454eb98f4e239341871580"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd41cef3692450b897619ca6d93dd3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b1328c8f29f4866b2a0d8076b725d4c"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    padding_side=\"left\", \n    add_eos_token=True,\n    add_bos_token=True,\n    use_fast=False\n)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:08:31.772747Z","iopub.execute_input":"2025-02-19T11:08:31.773173Z","iopub.status.idle":"2025-02-19T11:08:32.053714Z","shell.execute_reply.started":"2025-02-19T11:08:31.773139Z","shell.execute_reply":"2025-02-19T11:08:32.052873Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Ensure that the pynvml library is installed:\n# pip install nvidia-ml-py3\n\nfrom pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n\ndef print_gpu_utilization():\n    \"\"\"\n    Prints the GPU memory usage in MB.\n    \"\"\"\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used // (1024**2)} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:08:35.303921Z","iopub.execute_input":"2025-02-19T11:08:35.304212Z","iopub.status.idle":"2025-02-19T11:08:35.308482Z","shell.execute_reply.started":"2025-02-19T11:08:35.304189Z","shell.execute_reply":"2025-02-19T11:08:35.307556Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def gen(model, prompt, maxlen=100, sample=True):\n    # Tokenize the prompt and move tensors to GPU\n    toks = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    res = model.generate(\n        **toks,\n        max_new_tokens=maxlen,\n        do_sample=sample,\n        num_return_sequences=1,\n        temperature=0.1,\n        num_beams=1,\n        top_p=0.95,\n    )\n    generated_text = tokenizer.batch_decode(res, skip_special_tokens=True)[0]\n    \n    # Fix: Check if the expected delimiter exists before splitting\n    if \"Output:\\n\" in generated_text:\n        return generated_text.split(\"Output:\\n\", 1)[1]\n    else:\n        print(\"Warning: 'Output:' delimiter not found in the generated text.\")\n        return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:08:44.444041Z","iopub.execute_input":"2025-02-19T11:08:44.444353Z","iopub.status.idle":"2025-02-19T11:08:44.449390Z","shell.execute_reply.started":"2025-02-19T11:08:44.444330Z","shell.execute_reply":"2025-02-19T11:08:44.448483Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 10\n\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n# Call the generation function; it returns the output already processed.\noutput = gen(original_model, formatted_prompt, 100)\n\ndash_line = '-' * 100\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:08:46.669534Z","iopub.execute_input":"2025-02-19T11:08:46.669893Z","iopub.status.idle":"2025-02-19T11:08:49.942014Z","shell.execute_reply.started":"2025-02-19T11:08:46.669862Z","shell.execute_reply":"2025-02-19T11:08:49.941163Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n----------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n----------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nPerson1 and Person2 are at a party, and Person1 asks if they can have a dance. Person2 agrees and compliments Person1 on their appearance. Person1 thanks them and expresses their happiness with the party. Person2 agrees that it's a great party and suggests having a drink to celebrate.\n\nCPU times: user 2.64 s, sys: 225 ms, total: 2.87 s\nWall time: 3.27 s\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Formats the sample to create a prompt for training.\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    # Fix: Use get() to safely access dialogue and summary fields\n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = INSTRUCTION_KEY\n    input_context = sample.get(\"dialogue\", \"\")\n    response = f\"{RESPONSE_KEY}\\n{sample.get('summary', '')}\"\n    end = END_KEY\n    \n    # Join parts with double newlines for clarity\n    formatted_prompt = \"\\n\\n\".join([blurb, instruction, input_context, response, end])\n    sample[\"text\"] = formatted_prompt\n    return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:08:53.954012Z","iopub.execute_input":"2025-02-19T11:08:53.954392Z","iopub.status.idle":"2025-02-19T11:08:53.959202Z","shell.execute_reply.started":"2025-02-19T11:08:53.954354Z","shell.execute_reply":"2025-02-19T11:08:53.958268Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:08:57.298293Z","iopub.execute_input":"2025-02-19T11:08:57.298608Z","iopub.status.idle":"2025-02-19T11:08:57.304000Z","shell.execute_reply.started":"2025-02-19T11:08:57.298583Z","shell.execute_reply":"2025-02-19T11:08:57.303152Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from functools import partial\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:08:59.839610Z","iopub.execute_input":"2025-02-19T11:08:59.839978Z","iopub.status.idle":"2025-02-19T11:08:59.845166Z","shell.execute_reply.started":"2025-02-19T11:08:59.839952Z","shell.execute_reply":"2025-02-19T11:08:59.844247Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:09:02.426244Z","iopub.execute_input":"2025-02-19T11:09:02.426577Z","iopub.status.idle":"2025-02-19T11:09:02.431377Z","shell.execute_reply.started":"2025-02-19T11:09:02.426546Z","shell.execute_reply":"2025-02-19T11:09:02.430710Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 2488 MB\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ## Pre-process dataset\nmax_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\neval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:09:04.396948Z","iopub.execute_input":"2025-02-19T11:09:04.397274Z","iopub.status.idle":"2025-02-19T11:09:12.695305Z","shell.execute_reply.started":"2025-02-19T11:09:04.397244Z","shell.execute_reply":"2025-02-19T11:09:12.694590Z"}},"outputs":[{"name":"stdout","text":"Found max lenth: 2048\n2048\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d71f85f945c64f798c48fe4dbcc6199d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81360d2f8e454b71a89b651aeb3d295f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"684edf49dc7742eea479184c4225a45f"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40a1f589356d4cc2bb9c58bac50cc492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64da3527a11e47d69ea5f10919ecf703"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"992d19cee6704cb1815a9f66da4c163f"}},"metadata":{}},{"name":"stdout","text":"Shapes of the datasets:\nTraining: (1999, 3)\nValidation: (499, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 1999\n})\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:09:16.498430Z","iopub.execute_input":"2025-02-19T11:09:16.498816Z","iopub.status.idle":"2025-02-19T11:09:16.505565Z","shell.execute_reply.started":"2025-02-19T11:09:16.498784Z","shell.execute_reply":"2025-02-19T11:09:16.504487Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 262364160\nall model parameters: 1521392640\npercentage of trainable model parameters: 17.24%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(original_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:09:19.207292Z","iopub.execute_input":"2025-02-19T11:09:19.207681Z","iopub.status.idle":"2025-02-19T11:09:19.213710Z","shell.execute_reply.started":"2025-02-19T11:09:19.207608Z","shell.execute_reply":"2025-02-19T11:09:19.212725Z"}},"outputs":[{"name":"stdout","text":"PhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (rotary_emb): PhiRotaryEmbedding()\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:09:22.545372Z","iopub.execute_input":"2025-02-19T11:09:22.545741Z","iopub.status.idle":"2025-02-19T11:09:23.025597Z","shell.execute_reply.started":"2025-02-19T11:09:22.545708Z","shell.execute_reply":"2025-02-19T11:09:23.024698Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:09:27.556321Z","iopub.execute_input":"2025-02-19T11:09:27.556731Z","iopub.status.idle":"2025-02-19T11:09:27.565581Z","shell.execute_reply.started":"2025-02-19T11:09:27.556700Z","shell.execute_reply":"2025-02-19T11:09:27.564684Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 20971520\nall model parameters: 1542364160\npercentage of trainable model parameters: 1.36%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from transformers import TrainingArguments\n\noutput_dir = './peft-dialogue-summary-training/final-checkpoint'\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=4,            # For efficient GPU usage\n    gradient_accumulation_steps=2,              # To balance memory and speed\n    max_steps=500,                            # Assignment requires max_steps=500\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",                 # Optimizer for k-bit training\n    logging_steps=50,                         # Assignment requires logging_steps=50\n    save_strategy=\"steps\",\n    save_steps=50,                            # Save checkpoint every 50 steps\n    evaluation_strategy=\"steps\",\n    eval_steps=50,                            # Assignment requires eval_steps=50\n    do_eval=True,\n    gradient_checkpointing=False,             # Disabled during training for speed\n    fp16=True,                                # Mixed precision for faster training\n    report_to=\"none\",\n    overwrite_output_dir=True,\n    group_by_length=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:09:30.337641Z","iopub.execute_input":"2025-02-19T11:09:30.337975Z","iopub.status.idle":"2025-02-19T11:09:30.367005Z","shell.execute_reply.started":"2025-02-19T11:09:30.337947Z","shell.execute_reply":"2025-02-19T11:09:30.366253Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling, Trainer\n\n# Create a data collator for language modeling (mlm is set to False for causal models)\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n# Define the Trainer instance\npeft_trainer = Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=data_collator,\n)\n\npeft_training_args.device\npeft_trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:09:37.446208Z","iopub.execute_input":"2025-02-19T11:09:37.446499Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='102' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [102/500 16:02 < 1:03:48, 0.10 it/s, Epoch 0.40/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.422500</td>\n      <td>1.354080</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.289900</td>\n      <td>1.326990</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()  # Enter your HF token\n\n# Upload Model & Tokenizer\npeft_model.push_to_hub(\"sami1616/phi-2-dialogsum-finetuned\")\ntokenizer.push_to_hub(\"sami1616/phi-2-dialogsum-finetuned\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_gpu_utilization()\ndel original_model\ndel peft_trainer\ntorch.cuda.empty_cache()\nprint_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model_id = \"microsoft/phi-2\"\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 10\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    #print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rouge_score\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T10:35:25.715Z"}},"outputs":[],"execution_count":null}]}